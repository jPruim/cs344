{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1f1M1Zm07PLTvG3f6uN3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jPruim/cs344/blob/master/hw04/homework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM0YeHAWAguF",
        "colab_type": "text"
      },
      "source": [
        "Speculate on whether you believe that so-called “deep” neural networks are destined to be another bust just as perceptrons and expert systems were in the past, or whether they really are a breakthrough that will be used for years into the future. Please give a two-to-three-paragraph answer, including examples to back up your argument.\n",
        "\n",
        "Hand-compute a single, complete back-propagation cycle. Use the example network from class and compute the updated weight values for the first gradient descent iteration for the XOR example, i.e., [1, 1] → 0. Use the same initial weights we used in the class example but assume the identity function as the activation function (f(x) = x).\n",
        "\n",
        "Build a Keras-based ConvNet for Keras’s Fashion MNIST dataset (fashion_mnist). Experiment with different network architectures, submit your most performant network, and report the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6aCtGAEAnoA",
        "colab_type": "text"
      },
      "source": [
        "I believe that the future of deep neural networks is systematically and fundamentally better than the past future of perceptrons and expert systems. Perceptrons were exceedingly limited in what they could learn. Only solutions with linear separation between true and false could be found. Also, originally, there wasn't a systematic way of increasing the proficiency of the nodes by adding more layers. Expert systems had an even worse future outlook. They can only produce a mimicry of what humans already know. The goal of expert systems was to code in enough rules of thumb and heuristics such that a computer can mimic a human. This fails horribly because the limit of the program was what a coder understood about another person's knowledge.\n",
        "\n",
        "Deep neural networks truly shine at solving problems that programmers don't know how to code in classical methods. The computer finds an approximation to knowledge without the programmer knowing the answer in advance. Some key examples of this are AlphaGo and AlphaZero. Both took age old questions of how to play complicated board games well. And without any outside help besides the rules, learned to beat every player and algorithm that existed before them. It was even shown that AlphaGo added moves to the strategies of real life players with its unique and strong play style. These clearly show that the programmers don't need to know everythign in advance, and can even learn from the program's results.\n",
        "\n",
        "The other reason I think that the neural networks won't fall by the wayside is the use in industry. Overall, the image analysis that can be done allows for systematic and accurate acknowledgement of failed parts in industry. This then allows for fewer man-hours as a computer can slowly be used to replace people at portions of factory work. I saw an internship about this at Gentex. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvAu6Q9uPD1B",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "784FihHIQe99",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "0ac73ec9-1c40-4ee0-910f-4f6c95b98abb"
      },
      "source": [
        "import numpy as np\n",
        "answer = 1\n",
        "learningrate = 0.05\n",
        "o = np.array([0,1])\n",
        "def backprop(answer = 1, learningrate = 0.05, o=np.array([0,1])):\n",
        "  l1 = np.array([[.11,.12],[.21,.08]])\n",
        "  l2 = np.array([[.14],[.15]])\n",
        "  output = o.dot(l1.dot(l2))\n",
        "  print(output)\n",
        "  l2loss = (answer-output)**2\n",
        "  delta = answer - output\n",
        "  print(\"delta: \"+ str(delta))\n",
        "\n",
        "  #backpropagation\n",
        "  l2b = l2 + learningrate * ((o.dot(l1)).reshape(2,1)) * 1 * delta\n",
        "  print(\"new weights for layer 2: \"+str(l2b))\n",
        "  o2 = np.array([[o.item(0), o.item(0)],[o.item(1),o.item(1)]])\n",
        "  l1b = l1 + learningrate*delta * 1*np.multiply(o2,np.array([[0.14,0.15],[0.14,0.15]]))\n",
        "  print(\"new weights for layer 1: \" +str(l1b))\n",
        "backprop()\n",
        "backprop(answer=0,learningrate=0.05,o=np.array([1,1]))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0414]\n",
            "delta: [0.9586]\n",
            "new weights for layer 2: [[0.1500653]\n",
            " [0.1538344]]\n",
            "new weights for layer 1: [[0.11      0.12     ]\n",
            " [0.2167102 0.0871895]]\n",
            "[0.0748]\n",
            "delta: [-0.0748]\n",
            "new weights for layer 2: [[0.1388032]\n",
            " [0.149252 ]]\n",
            "new weights for layer 1: [[0.1094764 0.119439 ]\n",
            " [0.2094764 0.079439 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWt3anbKEAwb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhfDLozNEBKP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "bb697bd0-f0f8-483a-a23f-a359fa32cd14"
      },
      "source": [
        "#based on https://colab.research.google.com/github/margaretmz/deep-learning/blob/master/fashion_mnist_keras.ipynb and in class example\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(import_data, import_labels), (test_data, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 3us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmlxC80wGFuj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "548ff6d0-7b42-4a03-d4d8-aa9b13507a62"
      },
      "source": [
        "\n",
        "validation_data = import_data[50000:]\n",
        "validation_labels = import_labels[50000:]\n",
        "train_data = import_data[:50000]\n",
        "train_labels = import_labels[:50000]\n",
        "print(\"train_data:\", train_data.shape, \"train_labels:\", train_labels)\n",
        "print(\"validation_data:\", validation_data.shape, \"validation labels:\", validation_labels)\n",
        "# # Show one of the images from the training dataset\n",
        "#plt.imshow(train_data[img_index])\n",
        "train_data = train_data.reshape((50000, 28, 28, 1))\n",
        "train_data = train_data.astype('float32') / 255\n",
        "test_data = test_data.reshape(10000,28,28,1)\n",
        "test_data = test_data.astype('float32') / 255\n",
        "validation_data = validation_data.reshape(10000,28,28,1)\n",
        "validation_data = validation_data.astype('float32')/255\n",
        "\n",
        "model = models.Sequential()\n",
        "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=( 28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "print(model.summary())\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "validation_labels = to_categorical(validation_labels)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_data, train_labels, epochs=5, batch_size=64)\n",
        "model.evaluate(validation_data, validation_labels)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data: (50000, 28, 28) train_labels: [9 0 0 ... 5 1 7]\n",
            "validation_data: (10000, 28, 28) validation labels: [9 2 1 ... 3 0 5]\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "50000/50000 [==============================] - 40s 803us/step - loss: 0.5637 - accuracy: 0.7911\n",
            "Epoch 2/5\n",
            "50000/50000 [==============================] - 40s 796us/step - loss: 0.3466 - accuracy: 0.8733\n",
            "Epoch 3/5\n",
            "50000/50000 [==============================] - 39s 789us/step - loss: 0.2923 - accuracy: 0.8926\n",
            "Epoch 4/5\n",
            "50000/50000 [==============================] - 39s 785us/step - loss: 0.2578 - accuracy: 0.9053\n",
            "Epoch 5/5\n",
            "50000/50000 [==============================] - 39s 780us/step - loss: 0.2338 - accuracy: 0.9142\n",
            "10000/10000 [==============================] - 3s 254us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3405284868478775, 0.8815000057220459]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvYMlZJiMyCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "609f7ca5-9264-4d22-c623-fec0199f2e24"
      },
      "source": [
        "model2 = models.Sequential()\n",
        "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "model2.add(layers.Conv2D(32, (4, 4), activation='relu', input_shape=( 28, 28, 1)))\n",
        "model2.add(layers.MaxPooling2D((2, 2)))\n",
        "model2.add(layers.Conv2D(64, (4, 4), activation='relu'))\n",
        "model2.add(layers.MaxPooling2D((2, 2)))\n",
        "model2.add(layers.Conv2D(64, (4, 4), activation='relu'))\n",
        "\n",
        "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
        "model2.add(layers.Flatten())\n",
        "model2.add(layers.Dense(64, activation='relu'))\n",
        "model2.add(layers.Dense(10, activation='softmax'))\n",
        "print(model2.summary())\n",
        "\n",
        "model2.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model2.fit(train_data, train_labels, epochs=6, batch_size=100)\n",
        "model2.evaluate(validation_data, validation_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 25, 25, 32)        544       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 1, 1, 64)          65600     \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 103,786\n",
            "Trainable params: 103,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "50000/50000 [==============================] - 40s 797us/step - loss: 0.6916 - accuracy: 0.7391\n",
            "Epoch 2/6\n",
            "50000/50000 [==============================] - 40s 797us/step - loss: 0.4234 - accuracy: 0.8436\n",
            "Epoch 3/6\n",
            "50000/50000 [==============================] - 40s 796us/step - loss: 0.3468 - accuracy: 0.8718\n",
            "Epoch 4/6\n",
            "50000/50000 [==============================] - 40s 797us/step - loss: 0.3047 - accuracy: 0.8871\n",
            "Epoch 5/6\n",
            "50000/50000 [==============================] - 40s 794us/step - loss: 0.2770 - accuracy: 0.8976\n",
            "Epoch 6/6\n",
            "50000/50000 [==============================] - 40s 796us/step - loss: 0.2544 - accuracy: 0.9066\n",
            "10000/10000 [==============================] - 2s 239us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2769913217067719, 0.8974000215530396]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv9s35eRNkqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "4d218876-d863-443d-d620-a9bffbc0e7ec"
      },
      "source": [
        "model3 = models.Sequential()\n",
        "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "model3.add(layers.Conv2D(32, (2, 2), activation='relu', input_shape=( 28, 28, 1)))\n",
        "model3.add(layers.MaxPooling2D((2, 2)))\n",
        "model3.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
        "model3.add(layers.MaxPooling2D((2, 2)))\n",
        "model3.add(layers.Conv2D(64, (4, 4), activation='relu'))\n",
        "\n",
        "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
        "model3.add(layers.Flatten())\n",
        "model3.add(layers.Dense(128, activation='relu'))\n",
        "model3.add(layers.Dense(10, activation='softmax'))\n",
        "print(model3.summary())\n",
        "\n",
        "model3.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model3.fit(train_data, train_labels, epochs=6, batch_size=100)\n",
        "model3.evaluate(validation_data, validation_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 27, 27, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 12, 12, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 3, 3, 64)          65600     \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               73856     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 149,162\n",
            "Trainable params: 149,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "50000/50000 [==============================] - 34s 686us/step - loss: 0.6009 - accuracy: 0.7757\n",
            "Epoch 2/6\n",
            "50000/50000 [==============================] - 34s 679us/step - loss: 0.3514 - accuracy: 0.8720\n",
            "Epoch 3/6\n",
            "50000/50000 [==============================] - 34s 674us/step - loss: 0.2926 - accuracy: 0.8924\n",
            "Epoch 4/6\n",
            "50000/50000 [==============================] - 34s 674us/step - loss: 0.2606 - accuracy: 0.9030\n",
            "Epoch 5/6\n",
            "50000/50000 [==============================] - 34s 674us/step - loss: 0.2359 - accuracy: 0.9130\n",
            "Epoch 6/6\n",
            "50000/50000 [==============================] - 34s 673us/step - loss: 0.2139 - accuracy: 0.9211\n",
            "10000/10000 [==============================] - 2s 228us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26006494793891904, 0.9083999991416931]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}