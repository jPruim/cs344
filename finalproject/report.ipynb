{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDANQV2n8Cjv34X2w9IMCB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jPruim/cs344/blob/master/finalproject/report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxNRx2vWf1j0",
        "colab_type": "text"
      },
      "source": [
        "**Vision:**\n",
        "\n",
        "The goal of this project is to use neural networks to try and solve common linear algebra problems. This will involve using neural nets and CNN's to try and see how few nodes are necessary to solve some problems, and how accurately I can find these values.\n",
        "\n",
        "Specifically I hope to investigate determinants and eigenvalues for matrices. Determinants are single unique values for a given matrix. A determinant can describe properties such as whether the matrix is invertable or not. Should the determinant be zero, then there is no inverse of the matrix. This creates a whole area of linear algebra where people investigate extensions of inverses to all matrices (e.g. Penrose-Moore inverse). Eigenvalues are a very common measurement use in physics. It becomes important to get eigenvalues from matrices to see how forcing changes a system. Eigenvalues are also the basis of many different types of matrix decompositions. Similarly, eigenvalues are prevalent throughout the solving of various differential equations. Overall, they might be the single most interesting derived values from a given matrix. \n",
        "\n",
        "This project will be used to determine how well a NN can approximate these values. Mostly, this project is to satisfy personal curiosity in the complexity of finding these values. If they prove hard or impossible to approximate, it shows a more significant difference between the creativity of the human mind and the computational power of a computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TEQxs4of23E",
        "colab_type": "text"
      },
      "source": [
        "**Background:**\n",
        "\n",
        "Mostly this project will be a question of feasability and optimizing. Thus, the project will focus on simple neural networks and convolutional neural networks. Information about simply neural networks can be found in Artificial Intelligenc: A Modern Approach by Suart Russel and Peter Norvig (chapter 18). For an example of Convolutional Neural Networks visit: https://github.com/kvlinden-courses/cs344-code/blob/master/u10nn/keras-cnn.ipynb created by kvlinden.\n",
        "\n",
        "Some of the work in the project was also with different activation functions. For reason's elaborated on below, ReLU activation functions did not work well for this project. Thus, some attempts were made with varying other types of activation functions.\n",
        "\n",
        "Specifically, the project was tested with both Leaky ReLU and Tanh activation functions. Basic ReLU functions have a derivative of zero for negative values, and a derivative of 1 on positive values. Leaky ReLU's have a positive gradient on negative values (though still less than 1). This then allows models to attribute error when neurons do not ever activate. Tanh is hyperbolic tangent. This type of activation function has an assymptote towards a zero gradient as the value gets negative, but never truly reaches zero gradient. Thus, there will always be some editing of values to the nodes that do not activate (though it becomes nearly negligable if too negative).\n",
        "\n",
        "For more information on Leaky ReLU's visit: https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7\n",
        "\n",
        "For more information on Tanh activation functions visit: https://towardsdatascience.com/complete-guide-of-activation-functions-34076e95d044"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KySQHyBKoSyq",
        "colab_type": "text"
      },
      "source": [
        "**Implementaion:**\n",
        "\n",
        "The first model attempted was a simple neural network designed to work with square matrices of size n=7. The data was created using numpy random with a set size of 40000. The structure is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd_2Bw3FohzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#start modeling.\n",
        "#determinant modeling 1 with simply ReLu \n",
        "#6 computation layers  (and then the final layer)\n",
        "detmodel = models.Sequential()\n",
        "detmodel.add(layers.Dense(n**2, activation='relu',input_dim=n**2))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "# model.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(1, activation='relu'))\n",
        "detmodel.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "detmodel.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IfFsqbfzYO",
        "colab_type": "text"
      },
      "source": [
        "After doing some evaluation on the success of the neural network, it became clear that there are some problems. Most notably, the loss flattens out at a relatively high value (after 2-3 epochs). Then with some further investigation, it becomes apparent that the model is only (or almost always) returning 0. This means that the ReLU's are creating a dead neural network.\n",
        "\n",
        "Some more investigation into Dead ReLU's revealed this:\n",
        "\"Unfortunately, ReLU units can be fragile during training and can \"die\". For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be \"dead\" (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.\" (CS231 Stanford)\n",
        "\n",
        "Due to this issue, I followed the advice of other projects found here: https://github.com/keras-team/keras/issues/3687. The advice was to replace ReLU's with Leaky ReLU functions or Tanh activation functions. This project only found significant success with leaky ReLU's.\n",
        "\n",
        "This was then the second model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uote_xp6p5kS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#start modeling.\n",
        "#determinant \n",
        "detmodel2 = models.Sequential()\n",
        "\n",
        "detmodel2.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3),input_dim=n**2))\n",
        "detmodel2.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel2.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel2.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel2.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "# detmodel2.add(layers.Dense(50, activation='relu'))\n",
        "# detmodel2.add(layers.Dense(50, activation='relu'))\n",
        "detmodel2.add(layers.Dense(1, activation=LeakyReLU(alpha=0.5)))\n",
        "detmodel2.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "detmodel2.summary()\n",
        "# this has 156,948 parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCIt1IDWgIvS",
        "colab_type": "text"
      },
      "source": [
        "When this model was trained for 300 epochs, it managed to get an averaged squared error of 0.0665 on the training data. This looks great, but as expected with this many nodes, the model is overfitting to the training set. The L2 loss on the validation was 4.2. This is worse than just blindly guessing zero (that would be a loss of around 2.2 depending on the specific instance of the random dataset).\n",
        "\n",
        "Thus the next step was to elminate nodes and possibly limit epochs to reduce overfitting. However, whenever a model was found that successfully seemed to approach the training set determinants, the validation set determinants were horrible. Thus, the project was expanded to include Convolution neural networks. The hope was that similar to how determinants depend on submatrices, it could be that convolution neural networks could do a better job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmACSJkADljo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CNN model for determinant\n",
        "detCNNmodel = models.Sequential()\n",
        "detCNNmodel.add(layers.Conv2D((n-2)**2, (5, 5), activation=LeakyReLU(alpha=0.3), input_shape=(n, n,1)))\n",
        "# Add layers to flatten the 2D image and then normal leaky ReLU layers\n",
        "detCNNmodel.add(layers.Flatten())\n",
        "detCNNmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.add(layers.Dense(1, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "\n",
        "detCNNmodel.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhsrEqeDD_kK",
        "colab_type": "text"
      },
      "source": [
        "However, the convolutional neural networks also seemed to fall short of the desired outcome. They still cannot fit generic matrices, and seem to only overfit the initial training data.\n",
        "\n",
        "Thus, the predictions were tried with one additional method: Dropout. Dropout helps prevent overfitting by resetting random parameters at a certain rate each training cycle. For best success, it seems ideal to add dropout after each layer at a relatively high rate (20% to 50%) (Browniee). Also, some tentative research and examples show that having too large or too small of a batch size can lead to worse fitting. Too large can lead to overfitting the dataset, while too small can lead to never finding global optimization (Shen). \n",
        "\n",
        "Thus, I made another model in hopes of succeeding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tasM_5ixQTNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGrLV1oQRlNK",
        "colab_type": "text"
      },
      "source": [
        "Note: I lowered the dropout rate beneath 0.2 because at 0.2 it never learned on the training data. I believe this is showing a weakness and possible infeasability of this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENBIKpcSRx8m",
        "colab_type": "text"
      },
      "source": [
        "Using the same information and repeated trial on Eigenvalues, I got another model that seems to only be able to fit the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LVOHWlvTo_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eigenmodel = models.Sequential()\n",
        "\n",
        "# # Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(layers.Flatten())\n",
        "\n",
        "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
        "\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3),input_dim=n**2))\n",
        "eigenmodel.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "eigenmodel.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBj3PI63El1o",
        "colab_type": "text"
      },
      "source": [
        "**Results:**\n",
        "\n",
        "It seems like a Neural Network is not capable of finding the determinant of a matrix it was not trained on. It seems to be unable to find the algorithms behind determinants. It is possible this is due to the inablity to have a large enough training sample. With a 7x7 matrix, there are 49 numbers, so just to have a single data set with all combinations of positive or negative values in each element of the matrix would take 2^49 (500 billion) matrices. This is easily too large to handle on a computer like mine. \n",
        "\n",
        "Thus, I am reluctantly concluding that atleast these types of neural networks cannot compute determinants.\n",
        "With some research online, I found a lay-person that ran into my problem, and no descernable research in the feasability of the project.\n",
        "Link to the other project: https://stackoverflow.com/questions/46734134/how-to-approximate-the-determinant-with-keras\n",
        "\n",
        "With eigenvalues, I saw some tentative research on using complicate manual encodings of matrices which allow for decent results, but none of these methods are designed for smallish matrices (they are designed for matrices which don't fit in memory at a given time).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meosOi3bF4jf",
        "colab_type": "text"
      },
      "source": [
        "**Implications:**\n",
        "\n",
        "The implication of this project is showing the limitations of neural networks. It seems that despite recent advances in computing and deep learning, it is hard for a computer to generate algorithms (or approximations thereof). This shows an area where the human mind seems to be better off than a computer. Perhaps, we are still far away from computers replacing the need for mathematicians after all.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WS5ilmMgJQs",
        "colab_type": "text"
      },
      "source": [
        "**Citations:**\n",
        "\n",
        "Brownlee, Jason. “Dropout Regularization in Deep Learning Models With Keras.” Machine Learning Mastery, 13 Sept. 2019, machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/.\n",
        "\n",
        "*CS231n Convolutional Neural Networks for Visual Recognition*, Stanford, cs231n.github.io/neural-networks-1/#actfun.\n",
        "\n",
        "eleanora, and denfromufa “How to Approximate the Determinant with Keras.” Stack Overflow,stackoverflow.com/questions/46734134/how-to-approximate-the-determinant-with-keras.\n",
        "\n",
        "Keras-Team. “Predictions Are All Zero · Issue #3687 · Keras-Team/Keras.” GitHub, github.com/keras-team/keras/issues/3687.\n",
        "\n",
        "Liu, Danqing. “A Practical Guide to ReLU.” Medium, Medium, 30 Nov. 2017, medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7.\n",
        "\n",
        "Russell, Stuart J., and Peter Norvig. Artificial Intelligence: a Modern Approach. Pearson India Education Services Pvt. Ltd., 2018.\n",
        "\n",
        "Shen, Kevin. “Effect of Batch Size on Training Dynamics.” Medium, Mini Distill, 19 June 2018, medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e.\n",
        "\n",
        "VanDer Linden, Keith. “Kvlinden-Courses/cs344-Code.” GitHub, github.com/kvlinden-courses/cs344-code/blob/master/u10nn/keras-cnn.ipynb."
      ]
    }
  ]
}