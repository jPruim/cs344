{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinAlg03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsIahr4dHVs8sqf9drTore",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jPruim/cs344/blob/master/finalproject/Linear_Algebra_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JTlkJz7LeWC",
        "colab_type": "text"
      },
      "source": [
        "Proposal:\n",
        "Use neural networks to try and solve common and less common linear algebra problems. This will involve using neural nets and CNN's to try and see how few nodes are necessary to solve some problems.\n",
        "\n",
        "Specifically I hope to investigate determinants, eigenvalues, and inverses for matrices. Also, I hope to be able to attempt to do a decomposition. Part of the project is determining whether or not it is possible to do some of these approximations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Our_qENHQKdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "import numpy.linalg as linalg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLjwN7XsUhmf",
        "colab_type": "code",
        "outputId": "35590b5e-e34d-42e2-af82-6245cd523aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "#create datasets, normalized arrays of n^2 size.\n",
        "#Thus all values are between -1 and 1\n",
        "#No third set needed as the sets are random and different each time anyway\n",
        "n = 7\n",
        "train_set_size = 40000\n",
        "train_set=np.random.random(size=(train_set_size, n, n))*2-1\n",
        "\n",
        "val_set_size = 10000\n",
        "val_set=np.random.random(size=(val_set_size, n, n))*2-1\n",
        "\n",
        "#print shape\n",
        "print(train_set[:1])\n",
        "print(len(train_set))\n",
        "\n",
        "\n",
        "\n",
        "#Shaped for simple neural networks\n",
        "train_set2 = np.empty((train_set_size,n**2))\n",
        "for i in range(train_set_size):\n",
        "  train_set2[i] = np.ravel(train_set[i])\n",
        "train_set2.reshape(train_set_size,n**2,1)\n",
        "print(train_set2.shape)\n",
        "\n",
        "val_set2 = np.empty((val_set_size,n**2))\n",
        "for i in range(val_set_size):\n",
        "  val_set2[i] = np.ravel(val_set[i])\n",
        "val_set2.reshape(val_set_size,n**2,1)\n",
        "print(val_set2.shape)\n",
        "\n",
        "#Shaped for CNN neural networks\n",
        "train_set3 = train_set.reshape(train_set_size,n,n,1)\n",
        "val_set3 = val_set.reshape(val_set_size,n,n,1)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[-0.04373091 -0.61745869 -0.12409737 -0.74107924 -0.83637785\n",
            "    0.32079463 -0.25021075]\n",
            "  [ 0.07600896  0.82222931  0.40687249  0.17299387 -0.971019\n",
            "   -0.03378352 -0.94829831]\n",
            "  [-0.38450047 -0.64311001 -0.06316817  0.01006581 -0.70171439\n",
            "    0.80355225 -0.55963759]\n",
            "  [-0.9163142   0.87334135 -0.18536459 -0.69522956 -0.69123154\n",
            "    0.48754666 -0.4216813 ]\n",
            "  [ 0.75556176  0.67123854 -0.073078    0.97946388  0.14325928\n",
            "   -0.39541995 -0.81288345]\n",
            "  [-0.32863349 -0.55992979 -0.17680193  0.44581195 -0.86690555\n",
            "   -0.12833812 -0.1961278 ]\n",
            "  [ 0.87907713 -0.24926918  0.27159382 -0.60023955 -0.64500244\n",
            "   -0.7825694  -0.59805912]]]\n",
            "40000\n",
            "(40000, 49)\n",
            "(10000, 49)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSWuqnlXYM_u",
        "colab_type": "code",
        "outputId": "a75ca95a-6fee-46ba-9008-27a422613b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#generate determinant labels\n",
        "determinant_set_size = train_set_size\n",
        "determinant_set = np.empty((determinant_set_size,1))\n",
        "determinant_abs_set  = np.empty((determinant_set_size,1))\n",
        "for i in range(0,determinant_set_size): \n",
        "  determinant_set[i]=linalg.det(train_set[i])\n",
        "  determinant_abs_set[i] = abs(determinant_set[i])\n",
        "\n",
        "print(sum(determinant_abs_set) / len(determinant_set))\n",
        "val_determinant_set_size = val_set_size\n",
        "val_determinant_set = np.empty((val_set_size,1))\n",
        "for i in range(0,val_determinant_set_size): \n",
        "  val_determinant_set[i]= linalg.det(val_set[i])\n",
        "\n",
        "print(determinant_set.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.93716768]\n",
            "(40000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyTNtRurZtS9",
        "colab_type": "code",
        "outputId": "4f188ab8-44e6-48c4-c703-906e3f5627d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#generate eigenvalue labels\n",
        "eigenvalue_set_size = train_set_size\n",
        "eigenvalue_set = np.empty((eigenvalue_set_size,n))\n",
        "for i in range(0,train_set_size):\n",
        "  eigenValues= linalg.eigvals(train_set[i])\n",
        "  eigenvalue_set[i]= eigenValues\n",
        "\n",
        "val_eigenvalue_set_size = val_set_size\n",
        "val_eigenvalue_set = np.empty((val_eigenvalue_set_size,n))\n",
        "for i in range(0,val_eigenvalue_set_size): \n",
        "  eigenValues = linalg.eigvals(train_set[i])\n",
        "  val_eigenvalue_set[i]= eigenValues"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktg3KA1-aHlA",
        "colab_type": "code",
        "outputId": "b59cd9c6-f422-4856-9781-4b76858340cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "#start modeling.\n",
        "#determinant modeling 1 with simply ReLu \n",
        "#6 computation layers  (and then the final layer)\n",
        "detmodel = models.Sequential()\n",
        "\n",
        "detmodel.add(layers.Dense(n**2, activation='relu',input_dim=n**2))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(50, activation='relu'))\n",
        "# model.add(layers.Dense(50, activation='relu'))\n",
        "detmodel.add(layers.Dense(1, activation='relu'))\n",
        "detmodel.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "detmodel.summary()\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_119 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 50)                2500      \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_122 (Dense)            (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_123 (Dense)            (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_124 (Dense)            (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_125 (Dense)            (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 15,201\n",
            "Trainable params: 15,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWA5a9M8-6ny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "be376337-843d-45e9-80c3-d64e8bbf9de1"
      },
      "source": [
        "#Simple NN model for determinant with Relu\n",
        "detmodel.fit(train_set2, determinant_set, epochs=5, batch_size=100)\n",
        "detmodel.evaluate(val_set2, val_determinant_set)\n",
        "print(detmodel.predict(val_set2[:10]))\n",
        "print(val_determinant_set[:10])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 1s 21us/step - loss: 2.3481\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 1s 17us/step - loss: 2.3481\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 1s 17us/step - loss: 2.3481\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 1s 17us/step - loss: 2.3481\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 1s 17us/step - loss: 2.3481\n",
            "10000/10000 [==============================] - 0s 21us/step\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[ 3.73893636]\n",
            " [-2.38908039]\n",
            " [ 0.09039151]\n",
            " [-0.66248686]\n",
            " [-0.86043685]\n",
            " [ 0.37028895]\n",
            " [-0.04389581]\n",
            " [-0.88441543]\n",
            " [-2.34117211]\n",
            " [-0.23658771]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3_izNQpd9iA",
        "colab_type": "text"
      },
      "source": [
        "This simple initial model has a couple worrying features. Most notably, the loss flattens out at a relatively high value. Then with some further investigation, it becomes apparent that the model is only (or almost always) returning 0. This means that the ReLU's are dead.\n",
        "\n",
        "Some more investigation into Dead ReLU's revealed this:\n",
        "\"Unfortunately, ReLU units can be fragile during training and can \"die\". For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be \"dead\" (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.\" (CS231 Stanford)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGoy64OH-lgD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "7d55f904-68e4-4b62-eff5-ceafe9ee3300"
      },
      "source": [
        "#start modeling.\n",
        "#determinant \n",
        "detmodel2 = models.Sequential()\n",
        "\n",
        "detmodel2.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3),input_dim=n**2))\n",
        "detmodel2.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel2.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel2.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel2.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "# detmodel2.add(layers.Dense(50, activation='relu'))\n",
        "# detmodel2.add(layers.Dense(50, activation='relu'))\n",
        "detmodel2.add(layers.Dense(1, activation=LeakyReLU(alpha=0.5)))\n",
        "detmodel2.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "detmodel2.summary()\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_132 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_133 (Dense)            (None, 343)               17150     \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (None, 343)               117992    \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 49)                16856     \n",
            "_________________________________________________________________\n",
            "dense_136 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_137 (Dense)            (None, 1)                 50        \n",
            "=================================================================\n",
            "Total params: 156,948\n",
            "Trainable params: 156,948\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gklo_Z5Mi-Ww",
        "colab_type": "code",
        "outputId": "f9ddb9e2-f6d5-4964-e8de-90dd7343fc07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Simple NN model for determinant with Leaky RelU -- Overfits\n",
        "#The leaky ReLU's solve the Dead ReLU problem.\n",
        "detmodel2.fit(train_set2, determinant_set, epochs=300, batch_size=1000)\n",
        "\n",
        "print(\"\\n\\nEvaluation:\")\n",
        "print(detmodel2.evaluate(val_set2, val_determinant_set))\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Training data\")\n",
        "print(detmodel2.predict(train_set2[:5]))\n",
        "print(determinant_set[:5])\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Validation data\")\n",
        "print(detmodel2.predict(val_set2[:5]))\n",
        "print(val_determinant_set[:5])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 2.1496\n",
            "Epoch 2/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 2.1179\n",
            "Epoch 3/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 2.0893\n",
            "Epoch 4/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 2.0494\n",
            "Epoch 5/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 2.0058\n",
            "Epoch 6/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.9839\n",
            "Epoch 7/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.9411\n",
            "Epoch 8/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.8847\n",
            "Epoch 9/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.8588\n",
            "Epoch 10/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.8073\n",
            "Epoch 11/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.7579\n",
            "Epoch 12/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.7122\n",
            "Epoch 13/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.6745\n",
            "Epoch 14/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.6400\n",
            "Epoch 15/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.5965\n",
            "Epoch 16/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.5602\n",
            "Epoch 17/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.5044\n",
            "Epoch 18/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.4539\n",
            "Epoch 19/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.4152\n",
            "Epoch 20/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.3856\n",
            "Epoch 21/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.3814\n",
            "Epoch 22/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.3259\n",
            "Epoch 23/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.2808\n",
            "Epoch 24/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.2460\n",
            "Epoch 25/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.2139\n",
            "Epoch 26/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.1840\n",
            "Epoch 27/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.1758\n",
            "Epoch 28/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.1470\n",
            "Epoch 29/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.1149\n",
            "Epoch 30/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.0783\n",
            "Epoch 31/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.0621\n",
            "Epoch 32/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.0284\n",
            "Epoch 33/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 1.0082\n",
            "Epoch 34/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.9659\n",
            "Epoch 35/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.9574\n",
            "Epoch 36/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.9586\n",
            "Epoch 37/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.9376\n",
            "Epoch 38/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.8948\n",
            "Epoch 39/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.8771\n",
            "Epoch 40/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.8738\n",
            "Epoch 41/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.8390\n",
            "Epoch 42/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.8389\n",
            "Epoch 43/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.8024\n",
            "Epoch 44/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.7923\n",
            "Epoch 45/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.7780\n",
            "Epoch 46/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.7543\n",
            "Epoch 47/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.7511\n",
            "Epoch 48/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.7448\n",
            "Epoch 49/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.7134\n",
            "Epoch 50/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.7012\n",
            "Epoch 51/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.6785\n",
            "Epoch 52/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.6810\n",
            "Epoch 53/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.6473\n",
            "Epoch 54/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.6414\n",
            "Epoch 55/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.6509\n",
            "Epoch 56/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.6445\n",
            "Epoch 57/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.6138\n",
            "Epoch 58/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.5898\n",
            "Epoch 59/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.6091\n",
            "Epoch 60/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.5817\n",
            "Epoch 61/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.5780\n",
            "Epoch 62/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.5884\n",
            "Epoch 63/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.5624\n",
            "Epoch 64/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.5430\n",
            "Epoch 65/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.5078\n",
            "Epoch 66/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.5191\n",
            "Epoch 67/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.5049\n",
            "Epoch 68/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.5276\n",
            "Epoch 69/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.5447\n",
            "Epoch 70/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4716\n",
            "Epoch 71/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4802\n",
            "Epoch 72/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4613\n",
            "Epoch 73/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4666\n",
            "Epoch 74/300\n",
            "40000/40000 [==============================] - 1s 30us/step - loss: 0.4774\n",
            "Epoch 75/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4601\n",
            "Epoch 76/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4483\n",
            "Epoch 77/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.4313\n",
            "Epoch 78/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.4146\n",
            "Epoch 79/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4323\n",
            "Epoch 80/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4457\n",
            "Epoch 81/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4042\n",
            "Epoch 82/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.4071\n",
            "Epoch 83/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3761\n",
            "Epoch 84/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3848\n",
            "Epoch 85/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.3944\n",
            "Epoch 86/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3710\n",
            "Epoch 87/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3845\n",
            "Epoch 88/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3877\n",
            "Epoch 89/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3638\n",
            "Epoch 90/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3355\n",
            "Epoch 91/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.3413\n",
            "Epoch 92/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3353\n",
            "Epoch 93/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3311\n",
            "Epoch 94/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.3290\n",
            "Epoch 95/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3238\n",
            "Epoch 96/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3326\n",
            "Epoch 97/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.3357\n",
            "Epoch 98/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3381\n",
            "Epoch 99/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3135\n",
            "Epoch 100/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3086\n",
            "Epoch 101/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3195\n",
            "Epoch 102/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.3434\n",
            "Epoch 103/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.3145\n",
            "Epoch 104/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.3016\n",
            "Epoch 105/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2953\n",
            "Epoch 106/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3035\n",
            "Epoch 107/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3246\n",
            "Epoch 108/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.3146\n",
            "Epoch 109/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2855\n",
            "Epoch 110/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2791\n",
            "Epoch 111/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2552\n",
            "Epoch 112/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2539\n",
            "Epoch 113/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2723\n",
            "Epoch 114/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2766\n",
            "Epoch 115/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2560\n",
            "Epoch 116/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2501\n",
            "Epoch 117/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2622\n",
            "Epoch 118/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2521\n",
            "Epoch 119/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2558\n",
            "Epoch 120/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2502\n",
            "Epoch 121/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2263\n",
            "Epoch 122/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2323\n",
            "Epoch 123/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2404\n",
            "Epoch 124/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2308\n",
            "Epoch 125/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2281\n",
            "Epoch 126/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2296\n",
            "Epoch 127/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2308\n",
            "Epoch 128/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2469\n",
            "Epoch 129/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2304\n",
            "Epoch 130/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2119\n",
            "Epoch 131/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1953\n",
            "Epoch 132/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2074\n",
            "Epoch 133/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2128\n",
            "Epoch 134/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2078\n",
            "Epoch 135/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2055\n",
            "Epoch 136/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1987\n",
            "Epoch 137/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1960\n",
            "Epoch 138/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2137\n",
            "Epoch 139/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2045\n",
            "Epoch 140/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2154\n",
            "Epoch 141/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2102\n",
            "Epoch 142/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1874\n",
            "Epoch 143/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1957\n",
            "Epoch 144/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1825\n",
            "Epoch 145/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1928\n",
            "Epoch 146/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2038\n",
            "Epoch 147/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1787\n",
            "Epoch 148/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1662\n",
            "Epoch 149/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1773\n",
            "Epoch 150/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1878\n",
            "Epoch 151/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1736\n",
            "Epoch 152/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1878\n",
            "Epoch 153/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1948\n",
            "Epoch 154/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1863\n",
            "Epoch 155/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1975\n",
            "Epoch 156/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1663\n",
            "Epoch 157/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1570\n",
            "Epoch 158/300\n",
            "40000/40000 [==============================] - 1s 27us/step - loss: 0.1694\n",
            "Epoch 159/300\n",
            "40000/40000 [==============================] - 1s 27us/step - loss: 0.1600\n",
            "Epoch 160/300\n",
            "40000/40000 [==============================] - 1s 27us/step - loss: 0.1597\n",
            "Epoch 161/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1566\n",
            "Epoch 162/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1463\n",
            "Epoch 163/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2002\n",
            "Epoch 164/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1752\n",
            "Epoch 165/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1504\n",
            "Epoch 166/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1655\n",
            "Epoch 167/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.2088\n",
            "Epoch 168/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1530\n",
            "Epoch 169/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1390\n",
            "Epoch 170/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1399\n",
            "Epoch 171/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1332\n",
            "Epoch 172/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1367\n",
            "Epoch 173/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1420\n",
            "Epoch 174/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1539\n",
            "Epoch 175/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1404\n",
            "Epoch 176/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1465\n",
            "Epoch 177/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1592\n",
            "Epoch 178/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1449\n",
            "Epoch 179/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1536\n",
            "Epoch 180/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1444\n",
            "Epoch 181/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1481\n",
            "Epoch 182/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1515\n",
            "Epoch 183/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1324\n",
            "Epoch 184/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1302\n",
            "Epoch 185/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1468\n",
            "Epoch 186/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1837\n",
            "Epoch 187/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1793\n",
            "Epoch 188/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1488\n",
            "Epoch 189/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1443\n",
            "Epoch 190/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1409\n",
            "Epoch 191/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1251\n",
            "Epoch 192/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1120\n",
            "Epoch 193/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1056\n",
            "Epoch 194/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1168\n",
            "Epoch 195/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1241\n",
            "Epoch 196/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1306\n",
            "Epoch 197/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1303\n",
            "Epoch 198/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1571\n",
            "Epoch 199/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1244\n",
            "Epoch 200/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1197\n",
            "Epoch 201/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1360\n",
            "Epoch 202/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1212\n",
            "Epoch 203/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1217\n",
            "Epoch 204/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1183\n",
            "Epoch 205/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1190\n",
            "Epoch 206/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1262\n",
            "Epoch 207/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1166\n",
            "Epoch 208/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1257\n",
            "Epoch 209/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1145\n",
            "Epoch 210/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1201\n",
            "Epoch 211/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1251\n",
            "Epoch 212/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1307\n",
            "Epoch 213/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1112\n",
            "Epoch 214/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1079\n",
            "Epoch 215/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1068\n",
            "Epoch 216/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0962\n",
            "Epoch 217/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1238\n",
            "Epoch 218/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1174\n",
            "Epoch 219/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1126\n",
            "Epoch 220/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1258\n",
            "Epoch 221/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1145\n",
            "Epoch 222/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1084\n",
            "Epoch 223/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1019\n",
            "Epoch 224/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1237\n",
            "Epoch 225/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1218\n",
            "Epoch 226/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1078\n",
            "Epoch 227/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1168\n",
            "Epoch 228/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1149\n",
            "Epoch 229/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1000\n",
            "Epoch 230/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0977\n",
            "Epoch 231/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1137\n",
            "Epoch 232/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1059\n",
            "Epoch 233/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0963\n",
            "Epoch 234/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0954\n",
            "Epoch 235/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0977\n",
            "Epoch 236/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1070\n",
            "Epoch 237/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1214\n",
            "Epoch 238/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1096\n",
            "Epoch 239/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0892\n",
            "Epoch 240/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0908\n",
            "Epoch 241/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0924\n",
            "Epoch 242/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.2253\n",
            "Epoch 243/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1582\n",
            "Epoch 244/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1104\n",
            "Epoch 245/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0977\n",
            "Epoch 246/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0882\n",
            "Epoch 247/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0756\n",
            "Epoch 248/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0868\n",
            "Epoch 249/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0891\n",
            "Epoch 250/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0840\n",
            "Epoch 251/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0792\n",
            "Epoch 252/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0794\n",
            "Epoch 253/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0808\n",
            "Epoch 254/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0939\n",
            "Epoch 255/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0909\n",
            "Epoch 256/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0958\n",
            "Epoch 257/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0858\n",
            "Epoch 258/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0836\n",
            "Epoch 259/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0934\n",
            "Epoch 260/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1013\n",
            "Epoch 261/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0941\n",
            "Epoch 262/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0828\n",
            "Epoch 263/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0875\n",
            "Epoch 264/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1087\n",
            "Epoch 265/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1003\n",
            "Epoch 266/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0865\n",
            "Epoch 267/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0871\n",
            "Epoch 268/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0922\n",
            "Epoch 269/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1083\n",
            "Epoch 270/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1201\n",
            "Epoch 271/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1082\n",
            "Epoch 272/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0883\n",
            "Epoch 273/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0765\n",
            "Epoch 274/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0809\n",
            "Epoch 275/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1144\n",
            "Epoch 276/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1030\n",
            "Epoch 277/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0909\n",
            "Epoch 278/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0954\n",
            "Epoch 279/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0784\n",
            "Epoch 280/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0737\n",
            "Epoch 281/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0753\n",
            "Epoch 282/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0720\n",
            "Epoch 283/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1004\n",
            "Epoch 284/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0920\n",
            "Epoch 285/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0790\n",
            "Epoch 286/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.1063\n",
            "Epoch 287/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.1003\n",
            "Epoch 288/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0967\n",
            "Epoch 289/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0925\n",
            "Epoch 290/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0901\n",
            "Epoch 291/300\n",
            "40000/40000 [==============================] - 1s 30us/step - loss: 0.1007\n",
            "Epoch 292/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0687\n",
            "Epoch 293/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0688\n",
            "Epoch 294/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0664\n",
            "Epoch 295/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0716\n",
            "Epoch 296/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0916\n",
            "Epoch 297/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0881\n",
            "Epoch 298/300\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 0.0815\n",
            "Epoch 299/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0876\n",
            "Epoch 300/300\n",
            "40000/40000 [==============================] - 1s 29us/step - loss: 0.0665\n",
            "\n",
            "\n",
            "Evaluation:\n",
            "10000/10000 [==============================] - 0s 34us/step\n",
            "4.22254959526062\n",
            "\n",
            "\n",
            "Manual Comparison on Training data\n",
            "[[ 0.59268934]\n",
            " [ 3.0302517 ]\n",
            " [ 0.4314752 ]\n",
            " [ 0.38945642]\n",
            " [-1.5483649 ]]\n",
            "[[ 0.64717445]\n",
            " [ 3.60931389]\n",
            " [ 0.05669204]\n",
            " [ 0.81134262]\n",
            " [-1.58240957]]\n",
            "\n",
            "\n",
            "Manual Comparison on Validation data\n",
            "[[-2.584599  ]\n",
            " [-0.29590422]\n",
            " [-1.4135821 ]\n",
            " [ 2.863093  ]\n",
            " [-1.6785351 ]]\n",
            "[[ 3.73893636]\n",
            " [-2.38908039]\n",
            " [ 0.09039151]\n",
            " [-0.66248686]\n",
            " [-0.86043685]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ5QWfterBWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9aa902fa-f581-4707-a475-7b16bd2fb13b"
      },
      "source": [
        "#start modeling.\n",
        "#determinant modeling with fewer nodes and epochs,\n",
        "#Attempting to limit over fitting.\n",
        "#not working well at all\n",
        "detmodel3 = models.Sequential()\n",
        "\n",
        "detmodel3.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3),input_dim=n**2))\n",
        "detmodel3.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel3.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel3.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel3.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel3.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel3.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel3.add(layers.Dense(1, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel3.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "detmodel3.summary()\n",
        "\n",
        "\n",
        "#Simple NN model for determinant with Leaky RelU -- Overfits\n",
        "#The leaky ReLU's solve the Dead ReLU problem.\n",
        "detmodel3.fit(train_set2, determinant_set, epochs=100, batch_size=1000)\n",
        "\n",
        "print(\"\\n\\nEvaluation:\")\n",
        "print(detmodel3.evaluate(val_set2, val_determinant_set))\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Training data\")\n",
        "print(detmodel3.predict(train_set2[:5]))\n",
        "print(determinant_set[:5])\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Validation data\")\n",
        "print(detmodel3.predict(val_set2[:5]))\n",
        "print(val_determinant_set[:5])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_178 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_179 (Dense)            (None, 343)               17150     \n",
            "_________________________________________________________________\n",
            "dense_180 (Dense)            (None, 49)                16856     \n",
            "_________________________________________________________________\n",
            "dense_181 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_182 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_183 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_184 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_185 (Dense)            (None, 1)                 50        \n",
            "=================================================================\n",
            "Total params: 46,306\n",
            "Trainable params: 46,306\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 1s 19us/step - loss: 2.3495\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 2.3474\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.3446\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 2.3411\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.3372\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.3282\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.3196\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 2.3030\n",
            "Epoch 9/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.2908\n",
            "Epoch 10/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.2708\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 2.2517\n",
            "Epoch 12/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 2.2288\n",
            "Epoch 13/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.2115\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.1710\n",
            "Epoch 15/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 2.1429\n",
            "Epoch 16/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 2.1144\n",
            "Epoch 17/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.0924\n",
            "Epoch 18/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.0465\n",
            "Epoch 19/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 2.0229\n",
            "Epoch 20/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.9867\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.9554\n",
            "Epoch 22/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.9164\n",
            "Epoch 23/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.8872\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.8722\n",
            "Epoch 25/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.8434\n",
            "Epoch 26/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.8223\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.8170\n",
            "Epoch 28/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.7788\n",
            "Epoch 29/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.7419\n",
            "Epoch 30/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.7041\n",
            "Epoch 31/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.6903\n",
            "Epoch 32/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.6552\n",
            "Epoch 33/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.6270\n",
            "Epoch 34/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.6050\n",
            "Epoch 35/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.5932\n",
            "Epoch 36/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.5776\n",
            "Epoch 37/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.5412\n",
            "Epoch 38/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.5160\n",
            "Epoch 39/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.4989\n",
            "Epoch 40/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.4567\n",
            "Epoch 41/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.4375\n",
            "Epoch 42/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.4186\n",
            "Epoch 43/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.3989\n",
            "Epoch 44/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.3761\n",
            "Epoch 45/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.3829\n",
            "Epoch 46/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.3678\n",
            "Epoch 47/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.3135\n",
            "Epoch 48/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.2834\n",
            "Epoch 49/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.2763\n",
            "Epoch 50/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.2787\n",
            "Epoch 51/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.2464\n",
            "Epoch 52/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.2146\n",
            "Epoch 53/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.1865\n",
            "Epoch 54/100\n",
            "40000/40000 [==============================] - 1s 13us/step - loss: 1.1965\n",
            "Epoch 55/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.2024\n",
            "Epoch 56/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.1733\n",
            "Epoch 57/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.1375\n",
            "Epoch 58/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.1180\n",
            "Epoch 59/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.1202\n",
            "Epoch 60/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.1071\n",
            "Epoch 61/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.0904\n",
            "Epoch 62/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.0905\n",
            "Epoch 63/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.0540\n",
            "Epoch 64/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.0428\n",
            "Epoch 65/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 1.0546\n",
            "Epoch 66/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.0357\n",
            "Epoch 67/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.0191\n",
            "Epoch 68/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 1.0119\n",
            "Epoch 69/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 0.9959\n",
            "Epoch 70/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.9922\n",
            "Epoch 71/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.9781\n",
            "Epoch 72/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 0.9654\n",
            "Epoch 73/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.9556\n",
            "Epoch 74/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 0.9536\n",
            "Epoch 75/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 0.9444\n",
            "Epoch 76/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.9320\n",
            "Epoch 77/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 0.9284\n",
            "Epoch 78/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.9244\n",
            "Epoch 79/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.9284\n",
            "Epoch 80/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.9184\n",
            "Epoch 81/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.9009\n",
            "Epoch 82/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.9025\n",
            "Epoch 83/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8747\n",
            "Epoch 84/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8732\n",
            "Epoch 85/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8647\n",
            "Epoch 86/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8741\n",
            "Epoch 87/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8616\n",
            "Epoch 88/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8386\n",
            "Epoch 89/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8214\n",
            "Epoch 90/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8414\n",
            "Epoch 91/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8129\n",
            "Epoch 92/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 0.8148\n",
            "Epoch 93/100\n",
            "40000/40000 [==============================] - 1s 14us/step - loss: 0.8066\n",
            "Epoch 94/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.7998\n",
            "Epoch 95/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.7989\n",
            "Epoch 96/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.7899\n",
            "Epoch 97/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.8020\n",
            "Epoch 98/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.7884\n",
            "Epoch 99/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.7808\n",
            "Epoch 100/100\n",
            "40000/40000 [==============================] - 1s 15us/step - loss: 0.7683\n",
            "\n",
            "\n",
            "Evaluation:\n",
            "10000/10000 [==============================] - 0s 27us/step\n",
            "3.2944106355667113\n",
            "\n",
            "\n",
            "Manual Comparison on Training data\n",
            "[[ 1.1464698 ]\n",
            " [ 3.5409389 ]\n",
            " [-0.09306806]\n",
            " [ 1.3335532 ]\n",
            " [-0.25092682]]\n",
            "[[ 0.64717445]\n",
            " [ 3.60931389]\n",
            " [ 0.05669204]\n",
            " [ 0.81134262]\n",
            " [-1.58240957]]\n",
            "\n",
            "\n",
            "Manual Comparison on Validation data\n",
            "[[-0.41058868]\n",
            " [-0.24141304]\n",
            " [ 1.1401494 ]\n",
            " [-0.21008213]\n",
            " [ 0.31617805]]\n",
            "[[ 3.73893636]\n",
            " [-2.38908039]\n",
            " [ 0.09039151]\n",
            " [-0.66248686]\n",
            " [-0.86043685]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zjqDI2I_ikz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "222b027c-d85f-4213-cd94-3307cd15d320"
      },
      "source": [
        "#CNN model for determinant\n",
        "detCNNmodel = models.Sequential()\n",
        "detCNNmodel.add(layers.Conv2D((n-2)**2, (5, 5), activation=LeakyReLU(alpha=0.3), input_shape=(n, n,1)))\n",
        "# Add layers to flatten the 2D image and then normal leaky ReLU layers\n",
        "detCNNmodel.add(layers.Flatten())\n",
        "detCNNmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.add(layers.Dense(1, activation=LeakyReLU(alpha=0.3)))\n",
        "detCNNmodel.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "\n",
        "detCNNmodel.summary()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_49\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_25 (Conv2D)           (None, 3, 3, 25)          650       \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 225)               0         \n",
            "_________________________________________________________________\n",
            "dense_200 (Dense)            (None, 49)                11074     \n",
            "_________________________________________________________________\n",
            "dense_201 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_202 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_203 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_204 (Dense)            (None, 1)                 50        \n",
            "=================================================================\n",
            "Total params: 19,124\n",
            "Trainable params: 19,124\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MgrRcwuRurA",
        "colab_type": "code",
        "outputId": "4374021c-0762-42ff-c865-9dc29af56540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#CNN model for determinant\n",
        "detCNNmodel.fit(train_set3, determinant_set, epochs=50, batch_size=100)\n",
        "\n",
        "print(\"\\n\\nEvaluation:\")\n",
        "print(detCNNmodel.evaluate(val_set3, val_determinant_set))\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Training data\")\n",
        "print(detCNNmodel.predict(train_set3[:5]))\n",
        "print(determinant_set[:5])\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Validation data\")\n",
        "print(detCNNmodel.predict(val_set3[:5]))\n",
        "print(val_determinant_set[:5])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 2.3096\n",
            "Epoch 2/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 2.2967\n",
            "Epoch 3/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 2.2766\n",
            "Epoch 4/50\n",
            "40000/40000 [==============================] - 1s 27us/step - loss: 2.2619\n",
            "Epoch 5/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 2.2359\n",
            "Epoch 6/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 2.2179\n",
            "Epoch 7/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 2.1934\n",
            "Epoch 8/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 2.1615\n",
            "Epoch 9/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 2.1415\n",
            "Epoch 10/50\n",
            "40000/40000 [==============================] - 1s 27us/step - loss: 2.1103\n",
            "Epoch 11/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 2.0824\n",
            "Epoch 12/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 2.0685\n",
            "Epoch 13/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 2.0352\n",
            "Epoch 14/50\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 2.0085\n",
            "Epoch 15/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.9978\n",
            "Epoch 16/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.9645\n",
            "Epoch 17/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.9501\n",
            "Epoch 18/50\n",
            "40000/40000 [==============================] - 1s 28us/step - loss: 1.9147\n",
            "Epoch 19/50\n",
            "40000/40000 [==============================] - 1s 27us/step - loss: 1.9028\n",
            "Epoch 20/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.8751\n",
            "Epoch 21/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.8524\n",
            "Epoch 22/50\n",
            "40000/40000 [==============================] - 1s 27us/step - loss: 1.8296\n",
            "Epoch 23/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.8249\n",
            "Epoch 24/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.7992\n",
            "Epoch 25/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 1.7800\n",
            "Epoch 26/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 1.7689\n",
            "Epoch 27/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 1.7429\n",
            "Epoch 28/50\n",
            "40000/40000 [==============================] - 1s 27us/step - loss: 1.7221\n",
            "Epoch 29/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.7179\n",
            "Epoch 30/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.6988\n",
            "Epoch 31/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.6823\n",
            "Epoch 32/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 1.6742\n",
            "Epoch 33/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.6543\n",
            "Epoch 34/50\n",
            "40000/40000 [==============================] - 1s 27us/step - loss: 1.6485\n",
            "Epoch 35/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.6412\n",
            "Epoch 36/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.6153\n",
            "Epoch 37/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 1.6103\n",
            "Epoch 38/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 1.5989\n",
            "Epoch 39/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.5850\n",
            "Epoch 40/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.5789\n",
            "Epoch 41/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.5683\n",
            "Epoch 42/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 1.5441\n",
            "Epoch 43/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.5324\n",
            "Epoch 44/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.5308\n",
            "Epoch 45/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.5095\n",
            "Epoch 46/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.5161\n",
            "Epoch 47/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 1.5084\n",
            "Epoch 48/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.4974\n",
            "Epoch 49/50\n",
            "40000/40000 [==============================] - 1s 26us/step - loss: 1.4801\n",
            "Epoch 50/50\n",
            "40000/40000 [==============================] - 1s 25us/step - loss: 1.4626\n",
            "\n",
            "\n",
            "Evaluation:\n",
            "10000/10000 [==============================] - 0s 23us/step\n",
            "2.945627604484558\n",
            "\n",
            "\n",
            "Manual Comparison on Training data\n",
            "[[ 1.0623595 ]\n",
            " [ 2.438094  ]\n",
            " [-0.3056759 ]\n",
            " [ 0.42952877]\n",
            " [-2.353346  ]]\n",
            "[[ 0.64717445]\n",
            " [ 3.60931389]\n",
            " [ 0.05669204]\n",
            " [ 0.81134262]\n",
            " [-1.58240957]]\n",
            "\n",
            "\n",
            "Manual Comparison on Validation data\n",
            "[[-0.35223103]\n",
            " [-0.13404599]\n",
            " [-0.22670303]\n",
            " [-0.32163835]\n",
            " [-0.6266681 ]]\n",
            "[[ 3.73893636]\n",
            " [-2.38908039]\n",
            " [ 0.09039151]\n",
            " [-0.66248686]\n",
            " [-0.86043685]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0MaA41cKwzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad33b42d-41c2-404d-88e3-da5e18d8271e"
      },
      "source": [
        "#start modeling.\n",
        "#determinant modeling with dropout\n",
        "#Attempting to limit over fitting.\n",
        "\n",
        "detmodel4 = models.Sequential()\n",
        "\n",
        "detmodel4.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3),input_dim=n**2))\n",
        "detmodel4.add(Dropout(0.1))\n",
        "detmodel4.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel4.add(Dropout(0.1))\n",
        "detmodel4.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel4.add(Dropout(0.1))\n",
        "detmodel4.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel4.add(Dropout(0.1))\n",
        "detmodel4.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel4.add(layers.Dense(1, activation=LeakyReLU(alpha=0.3)))\n",
        "detmodel4.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "detmodel4.summary()\n",
        "\n",
        "\n",
        "#Simple NN model for determinant with Leaky RelU -- Overfits\n",
        "#The leaky ReLU's solve the Dead ReLU problem.\n",
        "detmodel4.fit(train_set2, determinant_set, epochs=80, batch_size=500)\n",
        "\n",
        "print(\"\\n\\nEvaluation:\")\n",
        "print(detmodel4.evaluate(val_set2, val_determinant_set))\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Training data\")\n",
        "print(detmodel4.predict(train_set2[:5]))\n",
        "print(determinant_set[:5])\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Validation data\")\n",
        "print(detmodel4.predict(val_set2[:5]))\n",
        "print(val_determinant_set[:5])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_61\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_259 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 49)                0         \n",
            "_________________________________________________________________\n",
            "dense_260 (Dense)            (None, 343)               17150     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 343)               0         \n",
            "_________________________________________________________________\n",
            "dense_261 (Dense)            (None, 343)               117992    \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 343)               0         \n",
            "_________________________________________________________________\n",
            "dense_262 (Dense)            (None, 49)                16856     \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 49)                0         \n",
            "_________________________________________________________________\n",
            "dense_263 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_264 (Dense)            (None, 1)                 50        \n",
            "=================================================================\n",
            "Total params: 156,948\n",
            "Trainable params: 156,948\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/80\n",
            "40000/40000 [==============================] - 2s 51us/step - loss: 2.3527\n",
            "Epoch 2/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 2.3474\n",
            "Epoch 3/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.3475\n",
            "Epoch 4/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.3450\n",
            "Epoch 5/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.3422\n",
            "Epoch 6/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.3402\n",
            "Epoch 7/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.3370\n",
            "Epoch 8/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.3333\n",
            "Epoch 9/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.3281\n",
            "Epoch 10/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.3238\n",
            "Epoch 11/80\n",
            "40000/40000 [==============================] - 2s 43us/step - loss: 2.3216\n",
            "Epoch 12/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.3153\n",
            "Epoch 13/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.3091\n",
            "Epoch 14/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.3054\n",
            "Epoch 15/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.2937\n",
            "Epoch 16/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.2951\n",
            "Epoch 17/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.2839\n",
            "Epoch 18/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.2809\n",
            "Epoch 19/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.2736\n",
            "Epoch 20/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 2.2611\n",
            "Epoch 21/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 2.2557\n",
            "Epoch 22/80\n",
            "40000/40000 [==============================] - 2s 47us/step - loss: 2.2411\n",
            "Epoch 23/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.2438\n",
            "Epoch 24/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.2378\n",
            "Epoch 25/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.2294\n",
            "Epoch 26/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.2145\n",
            "Epoch 27/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.2180\n",
            "Epoch 28/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.1991\n",
            "Epoch 29/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.1925\n",
            "Epoch 30/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.1855\n",
            "Epoch 31/80\n",
            "40000/40000 [==============================] - 2s 43us/step - loss: 2.1864\n",
            "Epoch 32/80\n",
            "40000/40000 [==============================] - 2s 43us/step - loss: 2.1732\n",
            "Epoch 33/80\n",
            "40000/40000 [==============================] - 2s 43us/step - loss: 2.1587\n",
            "Epoch 34/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.1505\n",
            "Epoch 35/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.1593\n",
            "Epoch 36/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 2.1475\n",
            "Epoch 37/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.1590\n",
            "Epoch 38/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.1423\n",
            "Epoch 39/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.1415\n",
            "Epoch 40/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.1170\n",
            "Epoch 41/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.1141\n",
            "Epoch 42/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.1086\n",
            "Epoch 43/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.0981\n",
            "Epoch 44/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 2.0949\n",
            "Epoch 45/80\n",
            "40000/40000 [==============================] - 2s 47us/step - loss: 2.0899\n",
            "Epoch 46/80\n",
            "40000/40000 [==============================] - 2s 47us/step - loss: 2.0764\n",
            "Epoch 47/80\n",
            "40000/40000 [==============================] - 2s 47us/step - loss: 2.0679\n",
            "Epoch 48/80\n",
            "40000/40000 [==============================] - 2s 48us/step - loss: 2.0652\n",
            "Epoch 49/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 2.0481\n",
            "Epoch 50/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.0613\n",
            "Epoch 51/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.0570\n",
            "Epoch 52/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.0532\n",
            "Epoch 53/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 2.0409\n",
            "Epoch 54/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.0391\n",
            "Epoch 55/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.0360\n",
            "Epoch 56/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 2.0294\n",
            "Epoch 57/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.0022\n",
            "Epoch 58/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.0105\n",
            "Epoch 59/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9933\n",
            "Epoch 60/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 2.0130\n",
            "Epoch 61/80\n",
            "40000/40000 [==============================] - 2s 47us/step - loss: 2.0015\n",
            "Epoch 62/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 1.9964\n",
            "Epoch 63/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 1.9881\n",
            "Epoch 64/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9953\n",
            "Epoch 65/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9903\n",
            "Epoch 66/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 1.9795\n",
            "Epoch 67/80\n",
            "40000/40000 [==============================] - 2s 46us/step - loss: 1.9738\n",
            "Epoch 68/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9642\n",
            "Epoch 69/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9569\n",
            "Epoch 70/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9554\n",
            "Epoch 71/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 1.9474\n",
            "Epoch 72/80\n",
            "40000/40000 [==============================] - 3s 67us/step - loss: 1.9531\n",
            "Epoch 73/80\n",
            "40000/40000 [==============================] - 3s 80us/step - loss: 1.9518\n",
            "Epoch 74/80\n",
            "40000/40000 [==============================] - 3s 82us/step - loss: 1.9323\n",
            "Epoch 75/80\n",
            "40000/40000 [==============================] - 3s 76us/step - loss: 1.9265\n",
            "Epoch 76/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9258\n",
            "Epoch 77/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9184\n",
            "Epoch 78/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9270\n",
            "Epoch 79/80\n",
            "40000/40000 [==============================] - 2s 44us/step - loss: 1.9139\n",
            "Epoch 80/80\n",
            "40000/40000 [==============================] - 2s 45us/step - loss: 1.9082\n",
            "\n",
            "\n",
            "Evaluation:\n",
            "10000/10000 [==============================] - 0s 39us/step\n",
            "2.3510844209671022\n",
            "\n",
            "\n",
            "Manual Comparison on Training data\n",
            "[[ 0.8310918 ]\n",
            " [ 3.8453538 ]\n",
            " [ 0.2769996 ]\n",
            " [ 0.0992999 ]\n",
            " [-0.21758817]]\n",
            "[[ 0.64717445]\n",
            " [ 3.60931389]\n",
            " [ 0.05669204]\n",
            " [ 0.81134262]\n",
            " [-1.58240957]]\n",
            "\n",
            "\n",
            "Manual Comparison on Validation data\n",
            "[[-0.20639013]\n",
            " [-0.27779824]\n",
            " [-0.09096948]\n",
            " [-0.01684698]\n",
            " [ 0.10774426]]\n",
            "[[ 3.73893636]\n",
            " [-2.38908039]\n",
            " [ 0.09039151]\n",
            " [-0.66248686]\n",
            " [-0.86043685]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW3C5Zjdw-bA",
        "colab_type": "code",
        "outputId": "1461de73-ade5-4e8e-9420-cef095e06d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Eigenvalue modeling\n",
        "print(eigenvalue_set.shape)\n",
        "\n",
        "eigenmodel = models.Sequential()\n",
        "\n",
        "# # Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(layers.Flatten())\n",
        "\n",
        "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
        "\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3),input_dim=n**2))\n",
        "eigenmodel.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**3, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n**2, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.add(layers.Dense(n, activation=LeakyReLU(alpha=0.3)))\n",
        "eigenmodel.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "eigenmodel.summary()\n",
        "eigenmodel.fit(train_set2, eigenvalue_set, epochs=80, batch_size=500)\n",
        "eigenmodel.evaluate(val_set2, val_eigenvalue_set)\n",
        "\n",
        "print(\"\\n\\nEvaluation:\")\n",
        "print(eigenmodel.evaluate(val_set2, val_eigenvalue_set))\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Training data\")\n",
        "print(eigenmodel.predict(train_set2[:5]))\n",
        "print(eigenvalue_set[:5])\n",
        "\n",
        "print(\"\\n\\nManual Comparison on Validation data\")\n",
        "print(eigenmodel.predict(val_set2[:5]))\n",
        "print(val_eigenvalue_set[:5])"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 7)\n",
            "Model: \"sequential_63\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_271 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_272 (Dense)            (None, 343)               17150     \n",
            "_________________________________________________________________\n",
            "dense_273 (Dense)            (None, 343)               117992    \n",
            "_________________________________________________________________\n",
            "dense_274 (Dense)            (None, 49)                16856     \n",
            "_________________________________________________________________\n",
            "dense_275 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_276 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_277 (Dense)            (None, 49)                2450      \n",
            "_________________________________________________________________\n",
            "dense_278 (Dense)            (None, 7)                 350       \n",
            "=================================================================\n",
            "Total params: 162,148\n",
            "Trainable params: 162,148\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "40000/40000 [==============================] - 2s 41us/step - loss: 0.7795\n",
            "Epoch 2/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7647\n",
            "Epoch 3/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7627\n",
            "Epoch 4/80\n",
            "40000/40000 [==============================] - 1s 35us/step - loss: 0.7610\n",
            "Epoch 5/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7598\n",
            "Epoch 6/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7580\n",
            "Epoch 7/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.7559\n",
            "Epoch 8/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7538\n",
            "Epoch 9/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7506\n",
            "Epoch 10/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7481\n",
            "Epoch 11/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7452\n",
            "Epoch 12/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7403\n",
            "Epoch 13/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.7365\n",
            "Epoch 14/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7324\n",
            "Epoch 15/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7273\n",
            "Epoch 16/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7192\n",
            "Epoch 17/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7124\n",
            "Epoch 18/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7061\n",
            "Epoch 19/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.7007\n",
            "Epoch 20/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.6907\n",
            "Epoch 21/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.6810\n",
            "Epoch 22/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.6766\n",
            "Epoch 23/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.6711\n",
            "Epoch 24/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.6590\n",
            "Epoch 25/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.6493\n",
            "Epoch 26/80\n",
            "40000/40000 [==============================] - 1s 35us/step - loss: 0.6420\n",
            "Epoch 27/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.6325\n",
            "Epoch 28/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.6240\n",
            "Epoch 29/80\n",
            "40000/40000 [==============================] - 1s 35us/step - loss: 0.6176\n",
            "Epoch 30/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.6073\n",
            "Epoch 31/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5984\n",
            "Epoch 32/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5912\n",
            "Epoch 33/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5823\n",
            "Epoch 34/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.5744\n",
            "Epoch 35/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.5682\n",
            "Epoch 36/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5594\n",
            "Epoch 37/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5501\n",
            "Epoch 38/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5465\n",
            "Epoch 39/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5351\n",
            "Epoch 40/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5343\n",
            "Epoch 41/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5276\n",
            "Epoch 42/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5221\n",
            "Epoch 43/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5133\n",
            "Epoch 44/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5083\n",
            "Epoch 45/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.5027\n",
            "Epoch 46/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4962\n",
            "Epoch 47/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4954\n",
            "Epoch 48/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4935\n",
            "Epoch 49/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4862\n",
            "Epoch 50/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4750\n",
            "Epoch 51/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4720\n",
            "Epoch 52/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4718\n",
            "Epoch 53/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4685\n",
            "Epoch 54/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4614\n",
            "Epoch 55/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4586\n",
            "Epoch 56/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4562\n",
            "Epoch 57/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4498\n",
            "Epoch 58/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4493\n",
            "Epoch 59/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4446\n",
            "Epoch 60/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4437\n",
            "Epoch 61/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4404\n",
            "Epoch 62/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4350\n",
            "Epoch 63/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4368\n",
            "Epoch 64/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4288\n",
            "Epoch 65/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4309\n",
            "Epoch 66/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4302\n",
            "Epoch 67/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4202\n",
            "Epoch 68/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4190\n",
            "Epoch 69/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4127\n",
            "Epoch 70/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4160\n",
            "Epoch 71/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4131\n",
            "Epoch 72/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4141\n",
            "Epoch 73/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4147\n",
            "Epoch 74/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4129\n",
            "Epoch 75/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4092\n",
            "Epoch 76/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4052\n",
            "Epoch 77/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4060\n",
            "Epoch 78/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.4034\n",
            "Epoch 79/80\n",
            "40000/40000 [==============================] - 1s 37us/step - loss: 0.4010\n",
            "Epoch 80/80\n",
            "40000/40000 [==============================] - 1s 36us/step - loss: 0.3976\n",
            "10000/10000 [==============================] - 0s 35us/step\n",
            "\n",
            "\n",
            "Evaluation:\n",
            "10000/10000 [==============================] - 0s 35us/step\n",
            "1.1795151990890502\n",
            "\n",
            "\n",
            "Manual Comparison on Training data\n",
            "[[-1.1395664e+00 -5.1652050e-01  4.1616234e-01  5.2489734e-01\n",
            "   4.8854828e-01  1.5146285e-04  6.7703426e-03]\n",
            " [-1.7213892e+00 -1.6791577e-02  1.0373862e+00  8.7713242e-01\n",
            "   5.0767148e-01  4.4125989e-01  4.8537087e-01]\n",
            " [-7.7396417e-01 -1.1031566e-02  6.8099928e-01  6.8203664e-01\n",
            "   6.7956042e-01  7.4479860e-01  7.2646195e-01]\n",
            " [ 1.4287909e+00  9.8046947e-01  2.4688467e-02 -2.4203229e-01\n",
            "  -4.2753047e-01 -2.9469964e-01 -2.8855097e-01]\n",
            " [-1.5502622e+00 -1.0997257e+00 -4.4579467e-01  4.4089459e-02\n",
            "   5.4792607e-01  6.3395482e-01  5.6854171e-01]]\n",
            "[[-1.17790363 -1.17790363  1.10113866  0.06065294  0.06065294  0.28516271\n",
            "   0.28516271]\n",
            " [-1.35271704 -1.35271704  0.62425263  0.62425263  1.53820119  1.11439752\n",
            "   0.33075326]\n",
            " [-0.80150308 -0.80150308  1.77651001  1.77651001  0.14917235  0.14917235\n",
            "   0.77202217]\n",
            " [ 1.55288097 -0.35817895 -0.35817895 -0.26112889 -0.26112889  0.4336812\n",
            "   0.4336812 ]\n",
            " [-1.746057   -1.19796087 -1.19796087 -0.1133362  -0.1133362   1.49897479\n",
            "   0.8596801 ]]\n",
            "\n",
            "\n",
            "Manual Comparison on Validation data\n",
            "[[-0.10529774 -0.21564296 -0.27776903 -0.13645728 -0.02225639 -0.00224886\n",
            "   0.0783253 ]\n",
            " [-1.3806498  -0.69455504 -0.1850565   0.21217737  0.72554183  0.8514012\n",
            "   0.75238305]\n",
            " [-0.22181244 -0.09544755  0.49111646  0.831128    0.90494     0.8282871\n",
            "   0.7973527 ]\n",
            " [-0.80603987 -0.8752752  -0.43674704 -0.05411972  0.36169255  0.48555574\n",
            "   0.40000325]\n",
            " [-0.80846345 -0.1788529   0.01124141  0.55626166  0.6520034   0.57698727\n",
            "   0.49204636]]\n",
            "[[-1.17790363 -1.17790363  1.10113866  0.06065294  0.06065294  0.28516271\n",
            "   0.28516271]\n",
            " [-1.35271704 -1.35271704  0.62425263  0.62425263  1.53820119  1.11439752\n",
            "   0.33075326]\n",
            " [-0.80150308 -0.80150308  1.77651001  1.77651001  0.14917235  0.14917235\n",
            "   0.77202217]\n",
            " [ 1.55288097 -0.35817895 -0.35817895 -0.26112889 -0.26112889  0.4336812\n",
            "   0.4336812 ]\n",
            " [-1.746057   -1.19796087 -1.19796087 -0.1133362  -0.1133362   1.49897479\n",
            "   0.8596801 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}